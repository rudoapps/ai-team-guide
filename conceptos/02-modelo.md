# Modelo

## ¬øQu√© es un Modelo?

**Definici√≥n simple:** Una implementaci√≥n espec√≠fica de un LLM, optimizada para ciertas tareas.

**Analog√≠a:** Si un LLM es el concepto de "motor de coche", un modelo es una implementaci√≥n espec√≠fica:
- **Modelo Econ√≥mico**: Eficiente, r√°pido, barato (GPT-3.5, Claude Instant)
- **Modelo de Lujo**: Alta calidad, caracter√≠sticas premium (GPT-4, Claude Opus)
- **Modelo Especializado**: Optimizado para tareas espec√≠ficas (Codex para c√≥digo)

## Qu√© Hace Diferentes a los Modelos

### Tama√±o y Entrenamiento
- **Modelos grandes**: M√°s datos, mejor calidad, m√°s caros, m√°s lentos
- **Modelos peque√±os**: M√°s r√°pidos, m√°s baratos, suficiente para tareas simples
- **Modelos especializados**: Entrenados en dominios espec√≠ficos

### Caracter√≠sticas Clave
- **Velocidad**: Qu√© tan r√°pido responden
- **Coste**: Cu√°nto cobran por uso (por token)
- **Precisi√≥n**: Qu√© tan fiables son sus resultados
- **Ventana de contexto**: Cu√°nto texto pueden procesar a la vez

## Modelos Populares para Desarrollo

### OpenAI GPT

**GPT-4 / GPT-4 Turbo**
- **Mejor para**: Tareas complejas, razonamiento avanzado
- **Ventana de contexto**: 8K - 128K tokens
- **Piensa en √©l como**: El consultor senior - caro pero muy capaz
- **Usar cuando**: La calidad es m√°s importante que el coste

**GPT-3.5 Turbo**
- **Mejor para**: Tareas cotidianas, respuestas r√°pidas
- **Ventana de contexto**: 16K tokens
- **Piensa en √©l como**: El generalista eficiente - buen balance
- **Usar cuando**: Necesitas rapidez y calidad decente

### Anthropic Claude

**Claude 3 Opus**
- **Mejor para**: An√°lisis profundo, tareas complejas
- **Ventana de contexto**: 200K tokens (~150 p√°ginas)
- **Piensa en √©l como**: El investigador meticuloso
- **Usar cuando**: Trabajas con documentos largos o necesitas precisi√≥n

**Claude 3 Sonnet**
- **Mejor para**: Balance de velocidad y calidad
- **Ventana de contexto**: 200K tokens
- **Piensa en √©l como**: El profesional vers√°til
- **Usar cuando**: Necesitas calidad con buen rendimiento

**Claude 3 Haiku**
- **Mejor para**: Tareas simples y r√°pidas
- **Ventana de contexto**: 200K tokens
- **Piensa en √©l como**: El asistente √°gil
- **Usar cuando**: Velocidad es prioridad

### Google Gemini

**Gemini Pro**
- **Mejor para**: Integraci√≥n con Google Workspace
- **Ventana de contexto**: 32K tokens
- **Piensa en √©l como**: El especialista del ecosistema Google
- **Usar cuando**: Tu stack incluye Gmail, Docs, Sheets

### Modelos Especializados

**GitHub Copilot / Codex**
- **Especializado en**: Generaci√≥n de c√≥digo
- **Usar para**: Autocompletar c√≥digo, sugerencias

**Claude Code**
- **Especializado en**: Agente de desarrollo
- **Usar para**: Tareas de desarrollo aut√≥nomas

## Eligiendo el Modelo Correcto

### √Årbol de Decisiones

**¬øNecesitas velocidad y bajo coste?**
‚Üí GPT-3.5 Turbo, Claude Haiku

**¬øLa calidad es cr√≠tica?**
‚Üí GPT-4, Claude Opus

**¬øTrabajas con documentos largos o muchos archivos?**
‚Üí Claude (200K tokens de contexto)

**¬øNecesitas razonamiento complejo?**
‚Üí GPT-4, Claude Opus

**¬øTarea espec√≠fica de c√≥digo?**
‚Üí GitHub Copilot, Claude Code

## Ejemplos Pr√°cticos por Rol

### Para Desarrolladores

**Generar funci√≥n simple:**
- Usa: GPT-3.5 Turbo (r√°pido, barato)

**Refactorizar proyecto completo:**
- Usa: Claude Opus (contexto grande, an√°lisis profundo)

**Debugging complejo:**
- Usa: GPT-4 o Claude Opus (razonamiento avanzado)

**Autocompletar mientras codeas:**
- Usa: GitHub Copilot

### Para Dise√±adores

**Naming conventions:**
- Usa: GPT-3.5 o Claude Haiku (tarea simple)

**Review de design system:**
- Usa: GPT-4 o Claude Opus (an√°lisis profundo)

**Documentaci√≥n de componentes:**
- Usa: Claude Sonnet (balance velocidad/calidad)

### Para Gesti√≥n

**Escribir correos:**
- Usa: GPT-3.5 (r√°pido, suficiente calidad)

**Analizar documentos largos:**
- Usa: Claude Opus (200K contexto)

**Crear documentaci√≥n t√©cnica:**
- Usa: GPT-4 o Claude Opus (precisi√≥n importante)

## Consideraciones de Coste

**Estructura de Costes T√≠pica:**
- **Tokens de entrada**: Lo que env√≠as
- **Tokens de salida**: Lo que recibes
- **Nivel del modelo**: Premium = 10-20x m√°s caro que b√°sico

**Consejos para Ahorrar:**
1. Usa modelos simples para tareas simples
2. S√© conciso en prompts
3. No regeneres innecesariamente
4. Cachea respuestas cuando sea posible

## Comparaci√≥n R√°pida

| Modelo | Velocidad | Coste | Calidad | Contexto | Mejor Para |
|--------|-----------|-------|---------|----------|------------|
| GPT-4 | ‚ö°‚ö° | üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 128K | Tareas complejas |
| GPT-3.5 | ‚ö°‚ö°‚ö°‚ö° | üí∞ | ‚≠ê‚≠ê‚≠ê | 16K | Uso general |
| Claude Opus | ‚ö°‚ö° | üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 200K | An√°lisis profundo |
| Claude Sonnet | ‚ö°‚ö°‚ö° | üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê | 200K | Balance |
| Claude Haiku | ‚ö°‚ö°‚ö°‚ö°‚ö° | üí∞ | ‚≠ê‚≠ê‚≠ê | 200K | Velocidad |

## Conclusi√≥n Clave

Piensa en los modelos como **diferentes especialistas en tu equipo**:
- El junior r√°pido para tareas simples
- El senior experto para trabajo complejo
- El especialista para necesidades espec√≠ficas

**Regla de oro:** Usa el modelo m√°s simple que cumpla tus requisitos de calidad.

## Relaci√≥n con Otros Conceptos

- **[LLM](./01-llm.md)**: Concepto general del que los modelos son implementaciones
- **[Token](./03-token.md)**: Los modelos cobran por token procesado
- **[Contexto](./04-contexto.md)**: Los modelos tienen l√≠mites de contexto diferentes

---

**Siguiente**: Aprende sobre [Tokens](./03-token.md) y c√≥mo afectan coste y rendimiento.

[‚Üê Volver a Conceptos](../README.md#conceptos)
